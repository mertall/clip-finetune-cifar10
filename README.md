# clip-finetune-cifar10
Finetuning CLIP with CIFAR-10 dataset with augmeneted image and text data.
## ğŸ“„ Notebook Walkthrough

The notebook is organized into eight main sections. Hereâ€™s what you can expect in each:

1. **Introduction & Objectives**  
   - **What youâ€™ll learn**: Fineâ€‘tuning CLIP, contrastive learning foundations, embedding extraction, HNSW indexing, and retrieval evaluation.  
   - **Motivation**: Why contrastive fineâ€‘tuning and fast similarity search matter for practical applications.

2. **Setup & Installation**  
   - **Environment**: Install required libraries (PyTorch,â€¯transformers,â€¯datasets,â€¯hnswlib, etc.).  
   - **Configuration**: Set device (CPU/GPU) and define helper imports.

3. **Data Visualization & Augmentation**  
   - **CIFARâ€‘10 Overview**: Load and display random image samples from each class.  
   - **Augmentation Pipeline**: Define and preview torchvision transforms (random crop, flip, color jitter).

4. **Dataset & DataLoader Definitions**  
   - **Contrastive Dataset**: Custom `CIFAR10Contrastive` class returns both image and text inputs per sample.  
   - **Synonym Expansion**: Enrich text prompts with templateâ€‘based synonyms for broader concept coverage.  
   - **Collate Function**: Batching logic to pad text sequences and stack image tensors.

5. **Training & Validation (Contrastive Learning)**  
   - **Model Setup**: Instantiate CLIP models (pretrained vs. fineâ€‘tuned).  
   - **Optimizer & Scheduler**: AdamW and learningâ€‘rate schedules (linear or cosine).  
   - **Training Loop**: Joint image/text loss, gradient accumulation, and epochâ€‘byâ€‘epoch logs.  
   - **Validation**: Zeroâ€‘shot accuracy using textâ€‘prototype matching.

6. **Embedding Extraction & Normalization**  
   - **Projection**: Extract final image and text embeddings from the model.  
   - **Normalization**: Lâ‚‚â€‘normalize vectors to unit length for cosine similarity.

7. **HNSW Index Construction & Querying**  
   - **Index Creation**: Build an HNSWLib index with combined embeddings.  
   - **Text â†’ Image**: Demonstrate ANN search with multimodal RAG.  
   - **Retrieval Evaluation**: Display topâ€‘k retrieved images for sample prompts.

8. **Advanced Evaluation & Visualization**  
   - **Recall/Precision@K**: Compute retrieval metrics against groundâ€‘truth labels.  
   - **Dimensionality Reduction**: UMAP builds its neighborhood graph from pairwise distances, which can become unreliable in very highâ€‘dimensional spaces. 
      By first applying PCA to reduce dimensionality and remove noisy, lowâ€‘variance directions, class separations become clearerâ€”enabling UMAP to produce more distinct, wellâ€‘separated clusters.
   - **Image Display**: Unfuzzify tensors back to viewable PIL images.

---

## ğŸ“Š Results & Insights

- Expect a significant lift in zeroâ€‘shot accuracy after fineâ€‘tuning (e.g., >9% vs. base CLIP).  
- Demonstrations of fast and accurate retrieval.  
- UMAP visualizations reveal clean class clusters in embedding space.

---

## ğŸ› ï¸ Extending this Work

- Swap CIFARâ€‘10 for a custom dataset by modifying the `CIFAR10Contrastive` class.  
- Experiment with different CLIP variants or backbone architectures.  
- Embeddings text and image in the same index allowing reverse search, first attempt failed- thinking its because non descriptive text was being used.